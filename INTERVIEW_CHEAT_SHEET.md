# üéØ Interview Cheat Sheet - Quick Reference

## üöÄ Startup Commands (Copy-Paste Ready)

```bash
# Terminal 1: Start Backend
cd /Users/shivanshsingh/Desktop/reddit_etl_project
docker-compose up -d
sleep 30  # Wait for services

# Terminal 2: Start Dashboard
cd /Users/shivanshsingh/Desktop/reddit_etl_project/dashboard
npm run dev

# Open in Browser
open http://localhost:3000
```

---

## üìä Key Numbers to Remember

| Metric | Value | What to Say |
|--------|-------|-------------|
| **Total Posts** | 701 | "Processing 701 Reddit posts" |
| **Subreddits** | 14 | "Across 14 Tesla-related communities" |
| **Entity Extraction** | 61.6% | "61.6% extraction rate for products/issues" |
| **Products Found** | 432 | "Identified 432 product mentions" |
| **Alerts** | 13 (5 CRITICAL) | "5 critical anomalies detected" |
| **Topics** | 15 keywords | "TF-IDF extracted 15 key topics" |
| **Clusters** | 7 groups | "K-Means identified 7 issue clusters" |
| **Avg Sentiment** | +0.250 | "Overall positive sentiment at +0.250" |

---

## üé§ 3-Minute Demo Script

### **Minute 1: Introduction + Dashboard**
"I built a real-time sentiment analysis pipeline for Tesla Energy discussions. It ingests Reddit posts via Kafka, processes them in Airflow, and visualizes insights in this Next.js dashboard. 

*[Show homepage]* 

We're currently analyzing 701 posts with 61.6% entity extraction. The sentiment timeline auto-refreshes every 30 seconds showing hourly trends."

### **Minute 2: Alerts + Products**
"The anomaly detection system flagged 5 critical alerts. 

*[Click Alerts, open one]* 

Each alert links to source posts and can be resolved with one click. 

*[Click Products]* 

We've categorized 432 product mentions - Model Y leads with 152 posts at +0.268 sentiment."

### **Minute 3: Insights + Architecture**
"*[Click Insights]* 

TF-IDF extracts keywords, K-Means clusters related issues. This cluster groups 87 solar-related posts. 

The stack is: Reddit API ‚Üí Kafka ‚Üí Airflow ‚Üí PostgreSQL ‚Üê Next.js. Designed to scale horizontally to millions of posts with connection pooling and efficient queries."

---

## üí° Anticipated Questions & Answers

### Q: "How do you handle late-arriving data?"

**A**: "Two approaches: (1) For batch processing, I use ON CONFLICT clauses in PostgreSQL for idempotent upserts. (2) For streaming, I'd implement watermarking in Spark with 10-minute grace periods to handle out-of-order events while balancing latency vs completeness."

### Q: "What if Reddit API goes down?"

**A**: "Circuit breaker pattern with exponential backoff (2, 4, 8, 16 seconds). After 5 failures, we enter degraded mode: cache recent posts, trigger alerts to PagerDuty, and retry every 5 minutes. Data quality checks in Airflow detect staleness within 1 hour."

### Q: "How do you ensure exactly-once processing?"

**A**: "Triple strategy: (1) Unique constraints on posts_raw.id (PRIMARY KEY) prevent duplicates. (2) Kafka consumer commits offsets after successful DB write. (3) Airflow task retries are idempotent - all INSERTs have ON CONFLICT DO NOTHING."

### Q: "How would you scale to millions of messages?"

**A**: "Horizontally scale each layer: (1) Kafka: partition by subreddit for parallel processing. (2) Airflow: CeleryExecutor with autoscaling workers. (3) PostgreSQL: read replicas + connection pooling (PgBouncer). (4) Dashboard: Next.js API routes are stateless - deploy to Cloud Run with auto-scaling."

### Q: "What about cost optimization?"

**A**: "Data lifecycle policies: hot data (7 days) in PostgreSQL, warm (90 days) in Parquet on GCS, cold (1 year) archived to Coldline Storage. Batch ETL for historical analysis, streaming only for real-time alerts. This saves ~70% on compute vs always-on streaming."

### Q: "How do you monitor data quality?"

**A**: "Four automated checks: (1) Schema validation with JSONSchema. (2) Null rate < 5% on key fields. (3) Duplicate detection. (4) Freshness alerts if no new data in 1 hour. Results stored in data_quality_metrics table with Airflow alerts on failure."

### Q: "Why VADER for sentiment analysis?"

**A**: "Trade-off decision: VADER is fast (1ms/post) and interpretable vs Hugging Face (50ms/post). At demo scale (700 posts), accuracy difference is minimal but VADER's 50x speedup matters at production scale (millions of posts). I'd A/B test both and pick based on accuracy vs cost metrics."

### Q: "How do you handle PII/sensitive data?"

**A**: "Reddit data is public, but the pipeline is PII-aware: (1) Regex filters for emails/SSNs. (2) Author names hashed with SHA-256. (3) PostgreSQL uses row-level security. (4) Dashboard API routes validate authorization tokens. For production, add data masking in staging environments."

---

## üèóÔ∏è Architecture Quick Summary

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Reddit API ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ HTTP
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Kafka Topic ‚îÇ ‚Üê Producer (Python)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ Stream
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Airflow   ‚îÇ ‚Üê Consumer + DAGs
‚îÇ  Scheduler  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ SQL
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PostgreSQL  ‚îÇ ‚Üê 6 tables
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ pg lib
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Next.js    ‚îÇ ‚Üê Dashboard
‚îÇ (localhost)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÅ Code Structure Walkthrough

If asked to show code:

### 1. **Data Ingestion** ‚Üí `reddit_kafka/producer.py`
"This fetches from Reddit API, publishes to Kafka with error handling and rate limiting."

### 2. **Sentiment Analysis** ‚Üí `utils/sentiment_analyzer.py`
"VADER assigns -1 to +1 sentiment scores, with compound scores aggregated."

### 3. **Entity Extraction** ‚Üí `utils/entity_extractor.py`
"Regex patterns extract vehicle models, energy products, and issues from post text."

### 4. **Anomaly Detection** ‚Üí `analytics/anomaly_detector.py`
"Statistical detection: sentiment drops > 2 std devs, volume spikes > 3x average."

### 5. **Topic Extraction** ‚Üí `analytics/topic_extractor.py`
"TF-IDF vectorizes text, K-Means clusters into 7 groups."

### 6. **Dashboard API** ‚Üí `dashboard/src/app/api/stats/route.ts`
"Next.js API routes query PostgreSQL, return JSON. Stateless for horizontal scaling."

---

## üîß Tech Stack Justification

| Choice | Why? |
|--------|------|
| **Kafka** | "Decouples producer/consumer, buffers during downtime, replay-able" |
| **Airflow** | "Visual DAGs, retry logic, scheduling, mature ecosystem" |
| **PostgreSQL** | "ACID compliance, JSONB for flexible schema, familiar SQL" |
| **Next.js** | "Server components reduce bundle size, API routes eliminate separate backend" |
| **TypeScript** | "Catches 80% of bugs at compile time, self-documenting code" |
| **VADER** | "Fast (1ms/post), no GPU needed, interpretable scores" |
| **Docker** | "Consistent dev/prod environments, easy to demo" |

---

## ‚ö†Ô∏è Known Limitations (Be Honest)

1. **Scale**: "Currently single-broker Kafka. Production needs 3+ brokers with replication factor 3."
2. **Auth**: "No authentication on dashboard. Would add NextAuth.js + JWT for production."
3. **Caching**: "No Redis yet. Adding it would cut API latency from 200ms to <20ms."
4. **Monitoring**: "Basic logging. Production needs Prometheus + Grafana dashboards."
5. **Testing**: "Unit tests exist but coverage is ~40%. Target is 80% before prod."

---

## üé® Features to Highlight

‚úÖ **Real-time**: Auto-refresh every 30s  
‚úÖ **Interactive**: Click alerts to see Reddit links  
‚úÖ **Responsive**: Works on mobile (demo by resizing window)  
‚úÖ **Type-safe**: Full TypeScript, zero `any` types in critical paths  
‚úÖ **Idempotent**: Can replay pipeline without duplicates  
‚úÖ **Monitorable**: Data quality checks in every DAG  
‚úÖ **Scalable**: Stateless design, horizontally scalable  
‚úÖ **Production-ready**: Error handling, logging, retries  

---

## üö® Emergency Recovery

### Dashboard won't start
```bash
cd dashboard
rm -rf .next node_modules
npm install
npm run dev
```

### No data showing
```bash
# Check database
docker exec -it postgres psql -U reddit_user -d reddit_pipeline -c "SELECT COUNT(*) FROM posts_raw;"

# If 0, trigger ingestion in Airflow UI
open http://localhost:8080
```

### Port conflict
```bash
lsof -ti:3000 | xargs kill -9
npm run dev
```

---

## ‚úÖ Pre-Demo Checklist

5 minutes before:
- [ ] `docker ps` shows 5 containers running
- [ ] http://localhost:3000 loads
- [ ] http://localhost:8080 (Airflow) accessible
- [ ] Browser zoom at 100%
- [ ] Close unnecessary tabs
- [ ] Terminal visible with `npm run dev` output
- [ ] Code editor ready with `dashboard/src/app/page.tsx`

---

## üéØ Closing Statement

"This project demonstrates production-grade data engineering: real-time ingestion, distributed processing, ML insights, and visualization. It's designed for Tesla's scale - the architecture handles millions of messages by adding Kafka partitions and Airflow workers. I'm excited to bring these skills to solving real problems like Powerwall telemetry analysis at Tesla Energy."

---

**Good luck! You've got this! üöÄ**

Last updated: November 21, 2024

